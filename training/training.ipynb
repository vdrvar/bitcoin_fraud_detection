{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Prepare Data for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "      <th>feature_165</th>\n",
       "      <th>feature_166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232438397</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232029206</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.005027</td>\n",
       "      <td>0.578941</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>4.380281</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>4.667146</td>\n",
       "      <td>0.851305</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.604120</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.333211</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232344069</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147852</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.137933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27553029</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.141519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.582077</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3881097</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172306</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>-0.163640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.054450</td>\n",
       "      <td>-1.760926</td>\n",
       "      <td>-1.760984</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0  232438397          1   0.163054   1.963790  -0.646376  12.409294   \n",
       "1  232029206          1  -0.005027   0.578941  -0.091383   4.380281   \n",
       "2  232344069          1  -0.147852  -0.184668  -1.201369  -0.121970   \n",
       "3   27553029          1  -0.151357  -0.184668  -1.201369  -0.121970   \n",
       "4    3881097          1  -0.172306  -0.184668  -1.201369   0.028105   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_158  feature_159  \\\n",
       "0  -0.063725   9.782742  12.414558  -0.163645  ...    -0.613614     0.241128   \n",
       "1  -0.063725   4.667146   0.851305  -0.163645  ...    -0.613614     0.241128   \n",
       "2  -0.043875  -0.113002  -0.061584  -0.137933  ...    -0.613614     0.241128   \n",
       "3  -0.043875  -0.113002  -0.061584  -0.141519  ...    -0.582077    -0.979074   \n",
       "4  -0.043875  -0.029140   0.242712  -0.163640  ...    -0.600999     0.241128   \n",
       "\n",
       "   feature_160  feature_161  feature_162  feature_163  feature_164  \\\n",
       "0     0.241406     1.072793     0.085530    -0.131155     0.677799   \n",
       "1     0.241406     0.604120     0.008632    -0.131155     0.333211   \n",
       "2     0.241406     0.018279    -0.087490    -0.131155    -0.097524   \n",
       "3    -0.978556     0.018279    -0.087490    -0.131155    -0.097524   \n",
       "4     0.241406     0.018279    -0.068266    -0.084674    -0.054450   \n",
       "\n",
       "   feature_165  feature_166  class  \n",
       "0    -0.120613    -0.119792      2  \n",
       "1    -0.120613    -0.119792      2  \n",
       "2    -0.120613    -0.119792      2  \n",
       "3    -0.120613    -0.119792      2  \n",
       "4    -1.760926    -1.760984      2  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the filtered datasets\n",
    "classes_df = pd.read_csv('../data/filtered/filtered_classes.csv')\n",
    "edgelist_df = pd.read_csv('../data/filtered/filtered_edgelist.csv')\n",
    "features_df = pd.read_csv('../data/filtered/filtered_features.csv', header=None)\n",
    "\n",
    "# Rename columns for features_df\n",
    "features_df.columns = ['txId'] + [f'feature_{i}' for i in range(1, features_df.shape[1])]\n",
    "\n",
    "# Merge features and classes data\n",
    "data = pd.merge(features_df, classes_df, on='txId')\n",
    "\n",
    "# Display the first few rows of the merged data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = data.drop(columns=['class'])\n",
    "y = data[['txId', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data\n",
    "# Convert edgelist to adjacency matrix\n",
    "G = nx.from_pandas_edgelist(edgelist_df, 'txId1', 'txId2')\n",
    "adj_matrix = nx.adjacency_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f657ebfe8e0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert adjacency matrix to edge index format\n",
    "edge_index, _ = from_scipy_sparse_matrix(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and labels into a single data object\n",
    "# Assuming features are sorted by txId\n",
    "features = torch.tensor(X.drop('txId', axis=1).values, dtype=torch.float)\n",
    "labels = torch.tensor(y['class'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=features, edge_index=edge_index, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[46564, 166], edge_index=[2, 73248], y=[46564])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prepared data object\n",
    "torch.save(data, 'graph_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.linear = torch.nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared data object\n",
    "data = torch.load('graph_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "num_node_features = data.num_features\n",
    "num_classes = len(torch.unique(data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_node_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(166, 16)\n",
      "  (conv2): GCNConv(16, 16)\n",
      "  (linear): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Adjust the labels to be in the range [0, 1]\n",
    "data.y = data.y - 1\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for train and test data\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[:int(data.num_nodes * 0.8)] = True\n",
    "data.test_mask = ~data.train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 0.7652, Test Accuracy: 0.9490\n",
      "Epoch 010, Loss: 0.3551, Test Accuracy: 0.9492\n",
      "Epoch 020, Loss: 0.3140, Test Accuracy: 0.9492\n",
      "Epoch 030, Loss: 0.2935, Test Accuracy: 0.9492\n",
      "Epoch 040, Loss: 0.2848, Test Accuracy: 0.9492\n",
      "Epoch 050, Loss: 0.2704, Test Accuracy: 0.9492\n",
      "Epoch 060, Loss: 0.2611, Test Accuracy: 0.9492\n",
      "Epoch 070, Loss: 0.2547, Test Accuracy: 0.9492\n",
      "Epoch 080, Loss: 0.2495, Test Accuracy: 0.9492\n",
      "Epoch 090, Loss: 0.2439, Test Accuracy: 0.9492\n",
      "Epoch 100, Loss: 0.2401, Test Accuracy: 0.9492\n",
      "Epoch 110, Loss: 0.2368, Test Accuracy: 0.9492\n",
      "Epoch 120, Loss: 0.2343, Test Accuracy: 0.9492\n",
      "Epoch 130, Loss: 0.2315, Test Accuracy: 0.9492\n",
      "Epoch 140, Loss: 0.2291, Test Accuracy: 0.9492\n",
      "Epoch 150, Loss: 0.2253, Test Accuracy: 0.9492\n",
      "Epoch 160, Loss: 0.2234, Test Accuracy: 0.9492\n",
      "Epoch 170, Loss: 0.2220, Test Accuracy: 0.9492\n",
      "Epoch 180, Loss: 0.2209, Test Accuracy: 0.9492\n",
      "Epoch 190, Loss: 0.2172, Test Accuracy: 0.9492\n",
      "Epoch 200, Loss: 0.2156, Test Accuracy: 0.9492\n",
      "Epoch 210, Loss: 0.2143, Test Accuracy: 0.9492\n",
      "Epoch 220, Loss: 0.2126, Test Accuracy: 0.9492\n",
      "Epoch 230, Loss: 0.2094, Test Accuracy: 0.9670\n",
      "Epoch 240, Loss: 0.2098, Test Accuracy: 0.9677\n",
      "Epoch 250, Loss: 0.2075, Test Accuracy: 0.9673\n",
      "Epoch 260, Loss: 0.2047, Test Accuracy: 0.9655\n",
      "Epoch 270, Loss: 0.2065, Test Accuracy: 0.9668\n",
      "Epoch 280, Loss: 0.2032, Test Accuracy: 0.9667\n",
      "Epoch 290, Loss: 0.2014, Test Accuracy: 0.9668\n",
      "Epoch 300, Loss: 0.2008, Test Accuracy: 0.9666\n",
      "Epoch 310, Loss: 0.1979, Test Accuracy: 0.9667\n",
      "Epoch 320, Loss: 0.1986, Test Accuracy: 0.9668\n",
      "Epoch 330, Loss: 0.1982, Test Accuracy: 0.9679\n",
      "Epoch 340, Loss: 0.1993, Test Accuracy: 0.9668\n",
      "Epoch 350, Loss: 0.1989, Test Accuracy: 0.9677\n",
      "Epoch 360, Loss: 0.1962, Test Accuracy: 0.9669\n",
      "Epoch 370, Loss: 0.1949, Test Accuracy: 0.9677\n",
      "Epoch 380, Loss: 0.1936, Test Accuracy: 0.9667\n",
      "Epoch 390, Loss: 0.2000, Test Accuracy: 0.9677\n",
      "Epoch 400, Loss: 0.1984, Test Accuracy: 0.9675\n",
      "Epoch 410, Loss: 0.1931, Test Accuracy: 0.9674\n",
      "Epoch 420, Loss: 0.1897, Test Accuracy: 0.9676\n",
      "Epoch 430, Loss: 0.1908, Test Accuracy: 0.9665\n",
      "Epoch 440, Loss: 0.1939, Test Accuracy: 0.9681\n",
      "Epoch 450, Loss: 0.1893, Test Accuracy: 0.9675\n",
      "Epoch 460, Loss: 0.1902, Test Accuracy: 0.9657\n",
      "Epoch 470, Loss: 0.1908, Test Accuracy: 0.9662\n",
      "Epoch 480, Loss: 0.1897, Test Accuracy: 0.9678\n",
      "Epoch 490, Loss: 0.1923, Test Accuracy: 0.9639\n",
      "Epoch 500, Loss: 0.1889, Test Accuracy: 0.9683\n",
      "Epoch 510, Loss: 0.1869, Test Accuracy: 0.9667\n",
      "Epoch 520, Loss: 0.1869, Test Accuracy: 0.9662\n",
      "Epoch 530, Loss: 0.1874, Test Accuracy: 0.9657\n",
      "Epoch 540, Loss: 0.1880, Test Accuracy: 0.9665\n",
      "Epoch 550, Loss: 0.1865, Test Accuracy: 0.9670\n",
      "Epoch 560, Loss: 0.1866, Test Accuracy: 0.9669\n",
      "Epoch 570, Loss: 0.1885, Test Accuracy: 0.9668\n",
      "Epoch 580, Loss: 0.1854, Test Accuracy: 0.9670\n",
      "Epoch 590, Loss: 0.1876, Test Accuracy: 0.9663\n",
      "Epoch 600, Loss: 0.1862, Test Accuracy: 0.9673\n",
      "Epoch 610, Loss: 0.1861, Test Accuracy: 0.9677\n",
      "Epoch 620, Loss: 0.1847, Test Accuracy: 0.9675\n",
      "Epoch 630, Loss: 0.1844, Test Accuracy: 0.9683\n",
      "Epoch 640, Loss: 0.1873, Test Accuracy: 0.9659\n",
      "Epoch 650, Loss: 0.1867, Test Accuracy: 0.9659\n",
      "Epoch 660, Loss: 0.1854, Test Accuracy: 0.9666\n",
      "Epoch 670, Loss: 0.1853, Test Accuracy: 0.9680\n",
      "Epoch 680, Loss: 0.1847, Test Accuracy: 0.9659\n",
      "Epoch 690, Loss: 0.1876, Test Accuracy: 0.9674\n",
      "Epoch 700, Loss: 0.1857, Test Accuracy: 0.9677\n",
      "Epoch 710, Loss: 0.1849, Test Accuracy: 0.9669\n",
      "Epoch 720, Loss: 0.1825, Test Accuracy: 0.9663\n",
      "Epoch 730, Loss: 0.1820, Test Accuracy: 0.9662\n",
      "Epoch 740, Loss: 0.1898, Test Accuracy: 0.9678\n",
      "Epoch 750, Loss: 0.1873, Test Accuracy: 0.9661\n",
      "Epoch 760, Loss: 0.1834, Test Accuracy: 0.9654\n",
      "Epoch 770, Loss: 0.1835, Test Accuracy: 0.9666\n",
      "Epoch 780, Loss: 0.1838, Test Accuracy: 0.9655\n",
      "Epoch 790, Loss: 0.1840, Test Accuracy: 0.9677\n",
      "Epoch 800, Loss: 0.1862, Test Accuracy: 0.9651\n",
      "Epoch 810, Loss: 0.1854, Test Accuracy: 0.9679\n",
      "Epoch 820, Loss: 0.1857, Test Accuracy: 0.9646\n",
      "Epoch 830, Loss: 0.1822, Test Accuracy: 0.9669\n",
      "Epoch 840, Loss: 0.1835, Test Accuracy: 0.9682\n",
      "Epoch 850, Loss: 0.1815, Test Accuracy: 0.9674\n",
      "Epoch 860, Loss: 0.1834, Test Accuracy: 0.9659\n",
      "Epoch 870, Loss: 0.1829, Test Accuracy: 0.9680\n",
      "Epoch 880, Loss: 0.1833, Test Accuracy: 0.9665\n",
      "Epoch 890, Loss: 0.1824, Test Accuracy: 0.9680\n",
      "Epoch 900, Loss: 0.1807, Test Accuracy: 0.9674\n",
      "Epoch 910, Loss: 0.1916, Test Accuracy: 0.9699\n",
      "Epoch 920, Loss: 0.1838, Test Accuracy: 0.9680\n",
      "Epoch 930, Loss: 0.1808, Test Accuracy: 0.9678\n",
      "Epoch 940, Loss: 0.1821, Test Accuracy: 0.9671\n",
      "Epoch 950, Loss: 0.1814, Test Accuracy: 0.9670\n",
      "Epoch 960, Loss: 0.1803, Test Accuracy: 0.9681\n",
      "Epoch 970, Loss: 0.1808, Test Accuracy: 0.9676\n",
      "Epoch 980, Loss: 0.1809, Test Accuracy: 0.9664\n",
      "Epoch 990, Loss: 0.1789, Test Accuracy: 0.9652\n",
      "Epoch 1000, Loss: 0.1885, Test Accuracy: 0.9640\n",
      "Epoch 1010, Loss: 0.1850, Test Accuracy: 0.9654\n",
      "Epoch 1020, Loss: 0.1857, Test Accuracy: 0.9651\n",
      "Epoch 1030, Loss: 0.1824, Test Accuracy: 0.9648\n",
      "Epoch 1040, Loss: 0.1822, Test Accuracy: 0.9613\n",
      "Epoch 1050, Loss: 0.1806, Test Accuracy: 0.9645\n",
      "Epoch 1060, Loss: 0.1828, Test Accuracy: 0.9666\n",
      "Epoch 1070, Loss: 0.1832, Test Accuracy: 0.9671\n",
      "Epoch 1080, Loss: 0.1831, Test Accuracy: 0.9657\n",
      "Epoch 1090, Loss: 0.1812, Test Accuracy: 0.9678\n",
      "Epoch 1100, Loss: 0.1826, Test Accuracy: 0.9649\n",
      "Epoch 1110, Loss: 0.1818, Test Accuracy: 0.9668\n",
      "Epoch 1120, Loss: 0.1787, Test Accuracy: 0.9663\n",
      "Epoch 1130, Loss: 0.1843, Test Accuracy: 0.9668\n",
      "Epoch 1140, Loss: 0.1808, Test Accuracy: 0.9652\n",
      "Epoch 1150, Loss: 0.1813, Test Accuracy: 0.9665\n",
      "Epoch 1160, Loss: 0.1778, Test Accuracy: 0.9671\n",
      "Epoch 1170, Loss: 0.1796, Test Accuracy: 0.9644\n",
      "Epoch 1180, Loss: 0.1800, Test Accuracy: 0.9648\n",
      "Epoch 1190, Loss: 0.1820, Test Accuracy: 0.9651\n",
      "Epoch 1200, Loss: 0.1836, Test Accuracy: 0.9657\n",
      "Epoch 1210, Loss: 0.1808, Test Accuracy: 0.9671\n",
      "Epoch 1220, Loss: 0.1785, Test Accuracy: 0.9647\n",
      "Epoch 1230, Loss: 0.1781, Test Accuracy: 0.9663\n",
      "Epoch 1240, Loss: 0.1806, Test Accuracy: 0.9645\n",
      "Epoch 1250, Loss: 0.1810, Test Accuracy: 0.9645\n",
      "Epoch 1260, Loss: 0.1805, Test Accuracy: 0.9675\n",
      "Epoch 1270, Loss: 0.1826, Test Accuracy: 0.9662\n",
      "Epoch 1280, Loss: 0.1837, Test Accuracy: 0.9620\n",
      "Epoch 1290, Loss: 0.1808, Test Accuracy: 0.9678\n",
      "Epoch 1300, Loss: 0.1822, Test Accuracy: 0.9648\n",
      "Epoch 1310, Loss: 0.1788, Test Accuracy: 0.9639\n",
      "Epoch 1320, Loss: 0.1781, Test Accuracy: 0.9655\n",
      "Epoch 1330, Loss: 0.1780, Test Accuracy: 0.9656\n",
      "Epoch 1340, Loss: 0.1823, Test Accuracy: 0.9656\n",
      "Epoch 1350, Loss: 0.1805, Test Accuracy: 0.9635\n",
      "Epoch 1360, Loss: 0.1825, Test Accuracy: 0.9610\n",
      "Epoch 1370, Loss: 0.1804, Test Accuracy: 0.9630\n",
      "Epoch 1380, Loss: 0.1772, Test Accuracy: 0.9651\n",
      "Epoch 1390, Loss: 0.1775, Test Accuracy: 0.9652\n",
      "Epoch 1400, Loss: 0.1790, Test Accuracy: 0.9637\n",
      "Epoch 1410, Loss: 0.1782, Test Accuracy: 0.9632\n",
      "Epoch 1420, Loss: 0.1803, Test Accuracy: 0.9625\n",
      "Epoch 1430, Loss: 0.1788, Test Accuracy: 0.9642\n",
      "Epoch 1440, Loss: 0.1813, Test Accuracy: 0.9669\n",
      "Epoch 1450, Loss: 0.1815, Test Accuracy: 0.9663\n",
      "Epoch 1460, Loss: 0.1786, Test Accuracy: 0.9656\n",
      "Epoch 1470, Loss: 0.1819, Test Accuracy: 0.9647\n",
      "Epoch 1480, Loss: 0.1803, Test Accuracy: 0.9659\n",
      "Epoch 1490, Loss: 0.1792, Test Accuracy: 0.9652\n",
      "Epoch 1500, Loss: 0.1771, Test Accuracy: 0.9648\n",
      "Epoch 1510, Loss: 0.1770, Test Accuracy: 0.9641\n",
      "Epoch 1520, Loss: 0.1777, Test Accuracy: 0.9647\n",
      "Epoch 1530, Loss: 0.1797, Test Accuracy: 0.9646\n",
      "Epoch 1540, Loss: 0.1794, Test Accuracy: 0.9660\n",
      "Epoch 1550, Loss: 0.1774, Test Accuracy: 0.9639\n",
      "Epoch 1560, Loss: 0.1813, Test Accuracy: 0.9661\n",
      "Epoch 1570, Loss: 0.1792, Test Accuracy: 0.9659\n",
      "Epoch 1580, Loss: 0.1785, Test Accuracy: 0.9650\n",
      "Epoch 1590, Loss: 0.1787, Test Accuracy: 0.9650\n",
      "Epoch 1600, Loss: 0.1812, Test Accuracy: 0.9638\n",
      "Epoch 1610, Loss: 0.1794, Test Accuracy: 0.9637\n",
      "Epoch 1620, Loss: 0.1783, Test Accuracy: 0.9649\n",
      "Epoch 1630, Loss: 0.1787, Test Accuracy: 0.9616\n",
      "Epoch 1640, Loss: 0.1773, Test Accuracy: 0.9632\n",
      "Epoch 1650, Loss: 0.1791, Test Accuracy: 0.9649\n",
      "Epoch 1660, Loss: 0.1781, Test Accuracy: 0.9633\n",
      "Epoch 1670, Loss: 0.1819, Test Accuracy: 0.9616\n",
      "Epoch 1680, Loss: 0.1761, Test Accuracy: 0.9628\n",
      "Epoch 1690, Loss: 0.1799, Test Accuracy: 0.9644\n",
      "Epoch 1700, Loss: 0.1798, Test Accuracy: 0.9644\n",
      "Epoch 1710, Loss: 0.1791, Test Accuracy: 0.9647\n",
      "Epoch 1720, Loss: 0.1811, Test Accuracy: 0.9654\n",
      "Epoch 1730, Loss: 0.1786, Test Accuracy: 0.9641\n",
      "Epoch 1740, Loss: 0.1785, Test Accuracy: 0.9642\n",
      "Epoch 1750, Loss: 0.1775, Test Accuracy: 0.9626\n",
      "Epoch 1760, Loss: 0.1824, Test Accuracy: 0.9656\n",
      "Epoch 1770, Loss: 0.1798, Test Accuracy: 0.9639\n",
      "Epoch 1780, Loss: 0.1772, Test Accuracy: 0.9652\n",
      "Epoch 1790, Loss: 0.1763, Test Accuracy: 0.9639\n",
      "Epoch 1800, Loss: 0.1775, Test Accuracy: 0.9650\n",
      "Epoch 1810, Loss: 0.1772, Test Accuracy: 0.9645\n",
      "Epoch 1820, Loss: 0.1843, Test Accuracy: 0.9603\n",
      "Epoch 1830, Loss: 0.1808, Test Accuracy: 0.9622\n",
      "Epoch 1840, Loss: 0.1765, Test Accuracy: 0.9636\n",
      "Epoch 1850, Loss: 0.1790, Test Accuracy: 0.9647\n",
      "Epoch 1860, Loss: 0.1773, Test Accuracy: 0.9646\n",
      "Epoch 1870, Loss: 0.1768, Test Accuracy: 0.9645\n",
      "Epoch 1880, Loss: 0.1787, Test Accuracy: 0.9656\n",
      "Epoch 1890, Loss: 0.1794, Test Accuracy: 0.9659\n",
      "Epoch 1900, Loss: 0.1793, Test Accuracy: 0.9648\n",
      "Epoch 1910, Loss: 0.1786, Test Accuracy: 0.9638\n",
      "Epoch 1920, Loss: 0.1765, Test Accuracy: 0.9662\n",
      "Epoch 1930, Loss: 0.1794, Test Accuracy: 0.9660\n",
      "Epoch 1940, Loss: 0.1808, Test Accuracy: 0.9654\n",
      "Epoch 1950, Loss: 0.1768, Test Accuracy: 0.9642\n",
      "Epoch 1960, Loss: 0.1784, Test Accuracy: 0.9653\n",
      "Epoch 1970, Loss: 0.1774, Test Accuracy: 0.9646\n",
      "Epoch 1980, Loss: 0.1789, Test Accuracy: 0.9637\n",
      "Epoch 1990, Loss: 0.1816, Test Accuracy: 0.9663\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        acc = evaluate()\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Test Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to 'gcn_model_weights.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model weights\n",
    "torch.save(model.state_dict(), 'gcn_model_weights.pth')\n",
    "print(\"Model weights saved to 'gcn_model_weights.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from 'gcn_model_weights.pth'\n"
     ]
    }
   ],
   "source": [
    "# Define the GCN model (make sure the model architecture matches the saved one)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.linear = torch.nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = GCN(num_node_features, num_classes)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('gcn_model_weights.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model weights loaded from 'gcn_model_weights.pth'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitcoin_fraud_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
